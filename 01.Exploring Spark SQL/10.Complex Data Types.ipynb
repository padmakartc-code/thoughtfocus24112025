{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "763ee9be-628f-43e7-9d4f-240389a29bca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Prerequisites\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d1c848b-959b-4dfc-9db6-9e11322ef744",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## load 'employee.csv' into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d79b559-335c-4423-ad96-9d0831e2b28b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\n",
    "    path = \"/Volumes/quickstart_catalog/quickstart_schema/sandbox/dataset/employee.csv\",\n",
    "                    sep = '|',\n",
    "                    header = True,\n",
    "                    inferSchema = True,\n",
    "                    quote = \"'\"\n",
    ").limit(10)\n",
    "\n",
    "df.printSchema()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad9ca5ad-6daa-4782-8ee6-5c4d893bcbdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Array Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e1f9754-c17f-4532-ab94-7d80fe2be3f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create an array type column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e83a535a-62b8-4682-bb9e-38f292f207f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split\n",
    " \n",
    "result_df = df.withColumn(\"skills\", split(\"col_skills\", \",\")).withColumn(\n",
    "    \"current_expected_salary\",\n",
    "    split(\"col_current_expected_salary\", \",\").cast(\"array<int>\"),\n",
    ")\n",
    "result_df.printSchema()\n",
    "result_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7642779a-6187-4661-ac1a-8fbfba2c02df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split\n",
    " \n",
    "result_df = (\n",
    "    df.withColumn(\"skills\", split(\"col_skills\", \",\"))\n",
    "    .withColumn(\n",
    "        \"current_expected_salary\",\n",
    "        split(\"col_current_expected_salary\", \",\").cast(\"array<int>\"),\n",
    "    )\n",
    "    .drop(\"col_skills\", \"col_current_expected_salary\")\n",
    ")\n",
    "result_df.printSchema()\n",
    "result_df.display()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da746558-9e90-43b2-82c0-65ec77e5adc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "result_df.select(\n",
    "    col(\"current_expected_salary\"),\n",
    "    col(\"current_expected_salary\")[0].alias(\"current_salary\"),\n",
    "    col(\"current_expected_salary\")[1].alias(\"expected_salary\"),\n",
    ").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09ab5a98-53a8-40e8-9051-24090484c74a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    " \n",
    "result_df1 = result_df.select(\n",
    "    col(\"name\"),\n",
    "    col(\"current_expected_salary\")[0].alias(\"current_salary\"),\n",
    "    col(\"current_expected_salary\")[1].alias(\"expected_salary\")\n",
    ")\n",
    " \n",
    "result_df1.filter(col(\"current_salary\") > col(\"expected_salary\")).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46fb8477-cc36-40d6-9498-ca50108b08f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Applying Different functions on array <<String>>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db432fe6-b544-4757-afab-5dc752d0be28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "775c8220-d8a1-48b6-8817-85b90a3ef36e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import size, array_distinct, array_contains\n",
    " \n",
    "result_df.select(\n",
    "    col(\"name\"),\n",
    "    col(\"skills\"),\n",
    "    size(\"skills\"),\n",
    "    array_contains(\"skills\", \"PySpark\"),\n",
    "    array_distinct(\"skills\").alias(\"distinct_skills\"),\n",
    ").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b62dfad-0a85-4114-b5e1-1024aeaf12c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import size, array_distinct, array_contains\n",
    " \n",
    "result_df2 = result_df.select(\n",
    "    col(\"name\"),\n",
    "    col(\"skills\"),\n",
    "    size(\"skills\"),\n",
    "    array_contains(\"skills\", \"PySpark\"),\n",
    "    array_distinct(\"skills\").alias(\"distinct_skills\"),\n",
    ")\n",
    "\n",
    "result_df2.filter(array_contains(\"skills\", \"PySpark\")).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6528af85-fa46-4c10-9507-e16aa1faeb7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, array_contains\n",
    "\n",
    "result_incr = (\n",
    "    result_df\n",
    "    .select(\n",
    "        col(\"name\"),\n",
    "        col(\"skills\"),\n",
    "        col(\"current_expected_salary\")[0].alias(\"current_salary\"),\n",
    "        col(\"current_expected_salary\")[1].alias(\"expected_salary\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"incr_salary\",\n",
    "        when(\n",
    "            array_contains(col(\"skills\"), \"PySpark\"),\n",
    "            col(\"current_salary\") * 1.30         \n",
    "        ).otherwise(col(\"current_salary\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "result_incr.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68181945-805c-4de3-849d-26f8ab0b66d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array_contains\n",
    " \n",
    "result_df.withColumn(\n",
    "    \"base_salary\",\n",
    "    when(\n",
    "        array_contains(\"skills\", \"PySpark\"), col(\"current_expected_salary\")[1] * 1.3\n",
    "    ).otherwise(col(\"current_expected_salary\")[1]),\n",
    ").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25f5ca3f-2fa4-4cb0-a272-56061a71e051",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, array_contains, when\n",
    " \n",
    "result_df = result_df.withColumn(\n",
    "    \"base_salary\",\n",
    "    when(\n",
    "        array_contains(col(\"skills\"), \"PySpark\"),\n",
    "        (col(\"current_expected_salary\")[1] * 1.3).cast(\"decimal(18,2)\")\n",
    "    ).otherwise(col(\"current_expected_salary\")[1].cast(\"decimal(18,2)\"))\n",
    ")\n",
    " \n",
    "result_df.display()\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "236e8387-dc10-4ee3-995f-e59e44b1e318",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Explode - It can be applied only on Array Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd2b8a7a-6eba-4b4f-b03d-fdf2f998e610",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "result_df.select(explode(\"skills\").alias(\"words\")).groupBy(\"words\").count().display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c05ead24-9bd0-4d73-984f-85d9ca75e5f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#STRUCT TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d2af512-32b0-47a4-a8bb-48fdc2b7f0c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Load Product_information_001.json into the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "746f7243-5b56-42de-be3d-6c24b36c4ffe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.json(\n",
    "    path=\"/Volumes/quickstart_catalog/quickstart_schema/sandbox/dataset/product_Information_001.json\",\n",
    "    multiLine=True,\n",
    ")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b829e3ea-53bd-4dc1-9142-d4d5e2e10970",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df.select(col(\"name\"), col(\"details.screen.size\")).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "547dbe96-9f4e-4768-904d-711dbe430ab0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df.select(col(\"product_id\"), col(\"name\"), col(\"details.screen.size\"),col(\"details.memory.size\"),col(\"details.storage.capacity\")).display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "10.Complex Data Types",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
